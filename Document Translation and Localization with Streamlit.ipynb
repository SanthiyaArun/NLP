{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Document Translation and Localization with Streamlit\n",
        "\n",
        "This Python script uses the Streamlit framework to create a simple web application for translating and localizing text in DOCX documents. The translation is performed using the Hugging Face Transformers library, and names are localized using spaCy.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Requirements](#requirements)\n",
        "2. [Installation](#installation)\n",
        "3. [Usage](#usage)\n",
        "4. [Code Explanation](#code-explanation)\n",
        "5. [License](#license)\n",
        "\n",
        "## Requirements <a name=\"requirements\"></a>\n",
        "\n",
        "- Python 3.x\n",
        "- Streamlit\n",
        "- transformers (from Hugging Face)\n",
        "- docx2txt\n",
        "- spaCy\n",
        "- docx (python-docx)\n",
        "\n",
        "## Installation <a name=\"installation\"></a>\n",
        "\n",
        "To install the required packages, you can use `pip`:\n",
        "\n",
        "```bash\n",
        "pip install streamlit transformers docx2txt spacy python-docx\n",
        "```\n",
        "\n",
        "Additionally, make sure you have a spaCy language model installed for the target country language. For example, if you're targeting French, you can install the French model like this:\n",
        "\n",
        "```bash\n",
        "python -m spacy download fr_core_news_sm\n",
        "```\n",
        "\n",
        "## Usage <a name=\"usage\"></a>\n",
        "\n",
        "1. Run the script using the following command:\n",
        "\n",
        "   ```bash\n",
        "   streamlit run your_script_name.py\n",
        "   ```\n",
        "\n",
        "2. Access the Streamlit app in your web browser.\n",
        "\n",
        "3. Upload a DOCX file that you want to translate and localize.\n",
        "\n",
        "4. Enter the target language (e.g., \"fr\" for French) and the target country code (e.g., \"fr\" for France).\n",
        "\n",
        "5. Click the \"Translate and Localize\" button.\n",
        "\n",
        "6. The app will process the uploaded document, perform translation, localize names, and generate a localized document.\n",
        "\n",
        "7. Download the localized document using the \"Download Localized Document\" button.\n",
        "\n",
        "## Code Explanation <a name=\"code-explanation\"></a>\n",
        "\n",
        "- The script begins by importing the necessary libraries and packages, including Streamlit, transformers (for translation), docx2txt (for extracting text from DOCX files), spaCy (for name localization), and python-docx (for working with DOCX documents).\n",
        "\n",
        "- Several functions are defined:\n",
        "  - `clean_text`: Cleans and tokenizes text by removing non-alphanumeric characters and converting it to lowercase.\n",
        "  - `translate_text`: Splits and translates text segments using a specified translation model from Hugging Face.\n",
        "  - `localize_names`: Localizes names in the translated document using spaCy and a predefined name localization database.\n",
        "\n",
        "- The Streamlit app is created, including a title and a file uploader for uploading DOCX files.\n",
        "\n",
        "- When a file is uploaded and the \"Translate and Localize\" button is clicked, the following steps are performed:\n",
        "  - The uploaded DOCX file is processed to extract its text content.\n",
        "  - User input for the target language and country code is obtained.\n",
        "  - The `clean_text` function is used to clean and tokenize the extracted text.\n",
        "  - The `translate_text` function translates the cleaned text to the target language.\n",
        "  - The `localize_names` function localizes names in the translated document.\n",
        "  - The styles, headers, and footers from the uploaded document are copied to a new localized document.\n",
        "  - The localized document is saved, and a download button is provided for the user to download it as a DOCX file.\n",
        "\n"
      ],
      "metadata": {
        "id": "RMRhYF66E0Z1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZevEh-xkQUN",
        "outputId": "45987ed4-aef4-4fa3-ed25-83791444d5c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-19 11:37:40.128 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import re\n",
        "from transformers import AutoTokenizer, MarianTokenizer, MarianMTModel\n",
        "import docx2txt\n",
        "import spacy\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
        "\n",
        "# Function to clean and tokenize text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Function to split and translate text segments\n",
        "def translate_text(input_text, target_language):\n",
        "    model_name = f\"Helsinki-NLP/opus-mt-en-{target_language}\"\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "    max_segment_length = 1152\n",
        "    segments = [input_text[i:i + max_segment_length] for i in range(0, len(input_text), max_segment_length)]\n",
        "\n",
        "    translated_segments = []\n",
        "\n",
        "    for segment in segments:\n",
        "        input_ids = tokenizer.encode(segment, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        translated_ids = model.generate(input_ids, max_length=100, num_beams=4, early_stopping=True)\n",
        "        translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "        translated_segments.append(translated_text)\n",
        "\n",
        "    return \"\\n\".join(translated_segments)\n",
        "\n",
        "# Function to localize names in the translated document\n",
        "def localize_names(translated_document, target_country_code):\n",
        "    nlp_target_lang = spacy.load(f\"{target_country_code}_core_news_sm\")\n",
        "\n",
        "    name_localization_db = {\n",
        "        \"fr\": {\n",
        "            \"John\": \"Jean\",\n",
        "            \"Marie\": \"Marie\",\n",
        "            # Add more name translations for France\n",
        "        },\n",
        "        \"de\": {\n",
        "            \"John\": \"Johann\",\n",
        "            \"Marie\": \"Maria\",\n",
        "            # Add more name translations for Germany\n",
        "        },\n",
        "        # Add more countries and their name translations as needed\n",
        "    }\n",
        "\n",
        "    target_language = name_localization_db.get(target_country_code, \"en\")\n",
        "\n",
        "    if target_language == \"fr\":\n",
        "        doc = nlp_target_lang(translated_document)\n",
        "        localized_names = name_localization_db[\"fr\"]\n",
        "    else:\n",
        "        doc = nlp_target_lang(translated_document)\n",
        "        localized_names = {}\n",
        "\n",
        "    localized_text = []\n",
        "    for token in doc:\n",
        "        if token.ent_type_ == \"PERSON\" and token.text in localized_names:\n",
        "            localized_text.append(localized_names[token.text])\n",
        "        else:\n",
        "            localized_text.append(token.text)\n",
        "\n",
        "    return \" \".join(localized_text)\n",
        "\n",
        "# Streamlit app\n",
        "st.title(\"Document Translation and Localization\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload a DOCX file\", type=[\"docx\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    doc_text = docx2txt.process(uploaded_file)\n",
        "    localized_doc = Document()\n",
        "\n",
        "    target_language = st.text_input(\"Enter the target language (e.g., fr for French):\")\n",
        "    target_country_code = st.text_input(\"Enter the target country code (e.g., fr for France):\")\n",
        "\n",
        "    if st.button(\"Translate and Localize\"):\n",
        "        cleaned_text = clean_text(doc_text)\n",
        "        translated_text = translate_text(cleaned_text, target_language)\n",
        "        localized_document = localize_names(translated_text, target_country_code)\n",
        "\n",
        "        # Copy styles, headers, and footers from the uploaded document\n",
        "        doc = Document(uploaded_file)\n",
        "        for element in doc.element.body:\n",
        "            if element.tag.endswith('sectPr'):\n",
        "                continue\n",
        "            localized_doc.element.body.append(element)\n",
        "\n",
        "        # Add the localized document to the Document layout\n",
        "        localized_doc.add_paragraph(localized_document, style='BodyText')\n",
        "        st.download_button(\n",
        "            \"Download Localized Document\",\n",
        "            lambda: localized_doc.save(\"localized_document.docx\"),\n",
        "            key=\"download-localized-docx\",\n",
        "            help=\"Click here to download the localized document as a DOCX file.\",\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raHRs7qkloOL",
        "outputId": "363ff76b-b709-4fdb-8b19-369a5132e06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.199.96:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/streamlit\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/bootstrap.py\", line 69, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/web/server/server.py\", line 395, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/streamlit/runtime/runtime.py\", line 325, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 798, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 515, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1wPYGCSkRTr",
        "outputId": "20d3fdd5-205c-468e-c2a7-e62997875033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.25.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.6)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.8.0)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.23.5)\n",
            "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
            "Requirement already satisfied: pillow<10,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=6.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Collecting pympler<2,>=0.9 (from streamlit)\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.18 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.5.2)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.7.1)\n",
            "Collecting tzlocal<5,>=1.1 (from streamlit)\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting validators<1,>=0.2 (from streamlit)\n",
            "  Downloading validators-0.21.2-py3-none-any.whl (25 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8 (from streamlit)\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.2)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit) (3.16.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.18->streamlit) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5,>=1.1->streamlit)\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.9.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, validators, tzdata, smmap, pympler, pytz-deprecation-shim, pydeck, gitdb, tzlocal, gitpython, streamlit\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.0.1\n",
            "    Uninstalling tzlocal-5.0.1:\n",
            "      Successfully uninstalled tzlocal-5.0.1\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.32 pydeck-0.8.0 pympler-1.0.1 pytz-deprecation-shim-0.1.0.post0 smmap-5.0.0 streamlit-1.25.0 tzdata-2023.3 tzlocal-4.3.1 validators-0.21.2 watchdog-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8qefv7AkWu6",
        "outputId": "5d500162-3fd8-43de-cc17-3944c1b76552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NtWRKMXkcb9",
        "outputId": "e7dfca39-28db-44e3-a1c3-87b611b7effe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=80fe3c78f9adac3f0387d266db7817e981e34f410b2d467726a89746669cfd72\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LYDH5c7kkQY",
        "outputId": "5376465a-3988-4d7b-c513-1cbae5ab88c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4C2meqUku5I",
        "outputId": "adb3ba6c-1942-40c5-827a-8874bdd64697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (4.9.3)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (9.4.0)\n",
            "Building wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53894 sha256=0f626728650f4a10a327e9cb055b36c9a887dc97aa54c803bb12a6110194c668\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/f5/1d/e09ba2c1907a43a4146d1189ae4733ca1a3bfe27ee39507767\n",
            "Successfully built docx\n",
            "Installing collected packages: docx\n",
            "Successfully installed docx-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx==0.8.11\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvj-GLt1lBnr",
        "outputId": "e13c4832-20f2-4ca6-a6ee-098facde086a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx==0.8.11\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx==0.8.11) (4.9.3)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=027e585fe0dd460183833484267b27da579a0fda13d8578a4dd711084e6e5c30\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9B2MTr5vlcX9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}